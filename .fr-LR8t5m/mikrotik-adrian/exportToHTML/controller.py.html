<html>
<head>
<title>controller.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.ln { color: #606366; font-weight: normal; font-style: normal; }
.s0 { color: rgb(98,151,85); font-style: italic; }
.s1 { color: rgb(169,183,198); }
.s2 { color: rgb(204,120,50); }
.s3 { color: rgb(106,135,89); }
.s4 { color: rgb(104,151,187); }
.s5 { color: rgb(128,128,128); }
</style>
</head>
<BODY BGCOLOR="#2b2b2b">
<TABLE CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<TR><TD><CENTER>
<FONT FACE="Arial, Helvetica" COLOR="#000000">
controller.py</FONT>
</center></TD></TR></TABLE>
<pre>
<span class="s0">&quot;&quot;&quot; 
The httplib2 algorithms ported for use with requests. 
&quot;&quot;&quot;</span><span class="s1"> 
</span><span class="s2">import </span><span class="s1">logging 
</span><span class="s2">import </span><span class="s1">re 
</span><span class="s2">import </span><span class="s1">calendar 
</span><span class="s2">import </span><span class="s1">time 
</span><span class="s2">from </span><span class="s1">email.utils </span><span class="s2">import </span><span class="s1">parsedate_tz 
 
</span><span class="s2">from </span><span class="s1">pip._vendor.requests.structures </span><span class="s2">import </span><span class="s1">CaseInsensitiveDict 
 
</span><span class="s2">from </span><span class="s1">.cache </span><span class="s2">import </span><span class="s1">DictCache 
</span><span class="s2">from </span><span class="s1">.serialize </span><span class="s2">import </span><span class="s1">Serializer 
 
 
logger = logging.getLogger(__name__) 
 
URI = re.compile(</span><span class="s3">r&quot;^(([^:/?#]+):)?(//([^/?#]*))?([^?#]*)(\?([^#]*))?(#(.*))?&quot;</span><span class="s1">) 
 
 
</span><span class="s2">def </span><span class="s1">parse_uri(uri): 
    </span><span class="s0">&quot;&quot;&quot;Parses a URI using the regex given in Appendix B of RFC 3986. 
 
        (scheme, authority, path, query, fragment) = parse_uri(uri) 
    &quot;&quot;&quot;</span><span class="s1"> 
    groups = URI.match(uri).groups() 
    </span><span class="s2">return </span><span class="s1">(groups[</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">groups[</span><span class="s4">3</span><span class="s1">]</span><span class="s2">, </span><span class="s1">groups[</span><span class="s4">4</span><span class="s1">]</span><span class="s2">, </span><span class="s1">groups[</span><span class="s4">6</span><span class="s1">]</span><span class="s2">, </span><span class="s1">groups[</span><span class="s4">8</span><span class="s1">]) 
 
 
</span><span class="s2">class </span><span class="s1">CacheController(object): 
    </span><span class="s0">&quot;&quot;&quot;An interface to see if request should cached or not. 
    &quot;&quot;&quot;</span><span class="s1"> 
    </span><span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">cache=</span><span class="s2">None, </span><span class="s1">cache_etags=</span><span class="s2">True, </span><span class="s1">serializer=</span><span class="s2">None</span><span class="s1">): 
        self.cache = cache </span><span class="s2">or </span><span class="s1">DictCache() 
        self.cache_etags = cache_etags 
        self.serializer = serializer </span><span class="s2">or </span><span class="s1">Serializer() 
 
    @classmethod 
    </span><span class="s2">def </span><span class="s1">_urlnorm(cls</span><span class="s2">, </span><span class="s1">uri): 
        </span><span class="s0">&quot;&quot;&quot;Normalize the URL to create a safe key for the cache&quot;&quot;&quot;</span><span class="s1"> 
        (scheme</span><span class="s2">, </span><span class="s1">authority</span><span class="s2">, </span><span class="s1">path</span><span class="s2">, </span><span class="s1">query</span><span class="s2">, </span><span class="s1">fragment) = parse_uri(uri) 
        </span><span class="s2">if not </span><span class="s1">scheme </span><span class="s2">or not </span><span class="s1">authority: 
            </span><span class="s2">raise </span><span class="s1">Exception(</span><span class="s3">&quot;Only absolute URIs are allowed. uri = %s&quot; </span><span class="s1">% uri) 
 
        scheme = scheme.lower() 
        authority = authority.lower() 
 
        </span><span class="s2">if not </span><span class="s1">path: 
            path = </span><span class="s3">&quot;/&quot;</span><span class="s1"> 
 
        </span><span class="s5"># Could do syntax based normalization of the URI before</span><span class="s1"> 
        </span><span class="s5"># computing the digest. See Section 6.2.2 of Std 66.</span><span class="s1"> 
        request_uri = query </span><span class="s2">and </span><span class="s3">&quot;?&quot;</span><span class="s1">.join([path</span><span class="s2">, </span><span class="s1">query]) </span><span class="s2">or </span><span class="s1">path 
        defrag_uri = scheme + </span><span class="s3">&quot;://&quot; </span><span class="s1">+ authority + request_uri 
 
        </span><span class="s2">return </span><span class="s1">defrag_uri 
 
    @classmethod 
    </span><span class="s2">def </span><span class="s1">cache_url(cls</span><span class="s2">, </span><span class="s1">uri): 
        </span><span class="s2">return </span><span class="s1">cls._urlnorm(uri) 
 
    </span><span class="s2">def </span><span class="s1">parse_cache_control(self</span><span class="s2">, </span><span class="s1">headers): 
        </span><span class="s0">&quot;&quot;&quot; 
        Parse the cache control headers returning a dictionary with values 
        for the different directives. 
        &quot;&quot;&quot;</span><span class="s1"> 
        retval = {} 
 
        cc_header = </span><span class="s3">'cache-control'</span><span class="s1"> 
        </span><span class="s2">if </span><span class="s3">'Cache-Control' </span><span class="s2">in </span><span class="s1">headers: 
            cc_header = </span><span class="s3">'Cache-Control'</span><span class="s1"> 
 
        </span><span class="s2">if </span><span class="s1">cc_header </span><span class="s2">in </span><span class="s1">headers: 
            parts = headers[cc_header].split(</span><span class="s3">','</span><span class="s1">) 
            parts_with_args = [ 
                tuple([x.strip().lower() </span><span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">part.split(</span><span class="s3">&quot;=&quot;</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)]) 
                </span><span class="s2">for </span><span class="s1">part </span><span class="s2">in </span><span class="s1">parts </span><span class="s2">if </span><span class="s1">-</span><span class="s4">1 </span><span class="s1">!= part.find(</span><span class="s3">&quot;=&quot;</span><span class="s1">) 
            ] 
            parts_wo_args = [ 
                (name.strip().lower()</span><span class="s2">, </span><span class="s4">1</span><span class="s1">) 
                </span><span class="s2">for </span><span class="s1">name </span><span class="s2">in </span><span class="s1">parts </span><span class="s2">if </span><span class="s1">-</span><span class="s4">1 </span><span class="s1">== name.find(</span><span class="s3">&quot;=&quot;</span><span class="s1">) 
            ] 
            retval = dict(parts_with_args + parts_wo_args) 
        </span><span class="s2">return </span><span class="s1">retval 
 
    </span><span class="s2">def </span><span class="s1">cached_request(self</span><span class="s2">, </span><span class="s1">request): 
        </span><span class="s0">&quot;&quot;&quot; 
        Return a cached response if it exists in the cache, otherwise 
        return False. 
        &quot;&quot;&quot;</span><span class="s1"> 
        cache_url = self.cache_url(request.url) 
        logger.debug(</span><span class="s3">'Looking up &quot;%s&quot; in the cache'</span><span class="s2">, </span><span class="s1">cache_url) 
        cc = self.parse_cache_control(request.headers) 
 
        </span><span class="s5"># Bail out if the request insists on fresh data</span><span class="s1"> 
        </span><span class="s2">if </span><span class="s3">'no-cache' </span><span class="s2">in </span><span class="s1">cc: 
            logger.debug(</span><span class="s3">'Request header has &quot;no-cache&quot;, cache bypassed'</span><span class="s1">) 
            </span><span class="s2">return False</span><span class="s1"> 
 
        </span><span class="s2">if </span><span class="s3">'max-age' </span><span class="s2">in </span><span class="s1">cc </span><span class="s2">and </span><span class="s1">cc[</span><span class="s3">'max-age'</span><span class="s1">] == </span><span class="s4">0</span><span class="s1">: 
            logger.debug(</span><span class="s3">'Request header has &quot;max_age&quot; as 0, cache bypassed'</span><span class="s1">) 
            </span><span class="s2">return False</span><span class="s1"> 
 
        </span><span class="s5"># Request allows serving from the cache, let's see if we find something</span><span class="s1"> 
        cache_data = self.cache.get(cache_url) 
        </span><span class="s2">if </span><span class="s1">cache_data </span><span class="s2">is None</span><span class="s1">: 
            logger.debug(</span><span class="s3">'No cache entry available'</span><span class="s1">) 
            </span><span class="s2">return False</span><span class="s1"> 
 
        </span><span class="s5"># Check whether it can be deserialized</span><span class="s1"> 
        resp = self.serializer.loads(request</span><span class="s2">, </span><span class="s1">cache_data) 
        </span><span class="s2">if not </span><span class="s1">resp: 
            logger.warning(</span><span class="s3">'Cache entry deserialization failed, entry ignored'</span><span class="s1">) 
            </span><span class="s2">return False</span><span class="s1"> 
 
        </span><span class="s5"># If we have a cached 301, return it immediately. We don't</span><span class="s1"> 
        </span><span class="s5"># need to test our response for other headers b/c it is</span><span class="s1"> 
        </span><span class="s5"># intrinsically &quot;cacheable&quot; as it is Permanent.</span><span class="s1"> 
        </span><span class="s5"># See:</span><span class="s1"> 
        </span><span class="s5">#   https://tools.ietf.org/html/rfc7231#section-6.4.2</span><span class="s1"> 
        </span><span class="s5">#</span><span class="s1"> 
        </span><span class="s5"># Client can try to refresh the value by repeating the request</span><span class="s1"> 
        </span><span class="s5"># with cache busting headers as usual (ie no-cache).</span><span class="s1"> 
        </span><span class="s2">if </span><span class="s1">resp.status == </span><span class="s4">301</span><span class="s1">: 
            msg = (</span><span class="s3">'Returning cached &quot;301 Moved Permanently&quot; response '</span><span class="s1"> 
                   </span><span class="s3">'(ignoring date and etag information)'</span><span class="s1">) 
            logger.debug(msg) 
            </span><span class="s2">return </span><span class="s1">resp 
 
        headers = CaseInsensitiveDict(resp.headers) 
        </span><span class="s2">if not </span><span class="s1">headers </span><span class="s2">or </span><span class="s3">'date' </span><span class="s2">not in </span><span class="s1">headers: 
            </span><span class="s2">if </span><span class="s3">'etag' </span><span class="s2">not in </span><span class="s1">headers: 
                </span><span class="s5"># Without date or etag, the cached response can never be used</span><span class="s1"> 
                </span><span class="s5"># and should be deleted.</span><span class="s1"> 
                logger.debug(</span><span class="s3">'Purging cached response: no date or etag'</span><span class="s1">) 
                self.cache.delete(cache_url) 
            logger.debug(</span><span class="s3">'Ignoring cached response: no date'</span><span class="s1">) 
            </span><span class="s2">return False</span><span class="s1"> 
 
        now = time.time() 
        date = calendar.timegm( 
            parsedate_tz(headers[</span><span class="s3">'date'</span><span class="s1">]) 
        ) 
        current_age = max(</span><span class="s4">0</span><span class="s2">, </span><span class="s1">now - date) 
        logger.debug(</span><span class="s3">'Current age based on date: %i'</span><span class="s2">, </span><span class="s1">current_age) 
 
        </span><span class="s5"># TODO: There is an assumption that the result will be a</span><span class="s1"> 
        </span><span class="s5">#       urllib3 response object. This may not be best since we</span><span class="s1"> 
        </span><span class="s5">#       could probably avoid instantiating or constructing the</span><span class="s1"> 
        </span><span class="s5">#       response until we know we need it.</span><span class="s1"> 
        resp_cc = self.parse_cache_control(headers) 
 
        </span><span class="s5"># determine freshness</span><span class="s1"> 
        freshness_lifetime = </span><span class="s4">0</span><span class="s1"> 
 
        </span><span class="s5"># Check the max-age pragma in the cache control header</span><span class="s1"> 
        </span><span class="s2">if </span><span class="s3">'max-age' </span><span class="s2">in </span><span class="s1">resp_cc </span><span class="s2">and </span><span class="s1">resp_cc[</span><span class="s3">'max-age'</span><span class="s1">].isdigit(): 
            freshness_lifetime = int(resp_cc[</span><span class="s3">'max-age'</span><span class="s1">]) 
            logger.debug(</span><span class="s3">'Freshness lifetime from max-age: %i'</span><span class="s2">,</span><span class="s1"> 
                         freshness_lifetime) 
 
        </span><span class="s5"># If there isn't a max-age, check for an expires header</span><span class="s1"> 
        </span><span class="s2">elif </span><span class="s3">'expires' </span><span class="s2">in </span><span class="s1">headers: 
            expires = parsedate_tz(headers[</span><span class="s3">'expires'</span><span class="s1">]) 
            </span><span class="s2">if </span><span class="s1">expires </span><span class="s2">is not None</span><span class="s1">: 
                expire_time = calendar.timegm(expires) - date 
                freshness_lifetime = max(</span><span class="s4">0</span><span class="s2">, </span><span class="s1">expire_time) 
                logger.debug(</span><span class="s3">&quot;Freshness lifetime from expires: %i&quot;</span><span class="s2">,</span><span class="s1"> 
                             freshness_lifetime) 
 
        </span><span class="s5"># Determine if we are setting freshness limit in the</span><span class="s1"> 
        </span><span class="s5"># request. Note, this overrides what was in the response.</span><span class="s1"> 
        </span><span class="s2">if </span><span class="s3">'max-age' </span><span class="s2">in </span><span class="s1">cc: 
            </span><span class="s2">try</span><span class="s1">: 
                freshness_lifetime = int(cc[</span><span class="s3">'max-age'</span><span class="s1">]) 
                logger.debug(</span><span class="s3">'Freshness lifetime from request max-age: %i'</span><span class="s2">,</span><span class="s1"> 
                             freshness_lifetime) 
            </span><span class="s2">except </span><span class="s1">ValueError: 
                freshness_lifetime = </span><span class="s4">0</span><span class="s1"> 
 
        </span><span class="s2">if </span><span class="s3">'min-fresh' </span><span class="s2">in </span><span class="s1">cc: 
            </span><span class="s2">try</span><span class="s1">: 
                min_fresh = int(cc[</span><span class="s3">'min-fresh'</span><span class="s1">]) 
            </span><span class="s2">except </span><span class="s1">ValueError: 
                min_fresh = </span><span class="s4">0</span><span class="s1"> 
            </span><span class="s5"># adjust our current age by our min fresh</span><span class="s1"> 
            current_age += min_fresh 
            logger.debug(</span><span class="s3">'Adjusted current age from min-fresh: %i'</span><span class="s2">,</span><span class="s1"> 
                         current_age) 
 
        </span><span class="s5"># Return entry if it is fresh enough</span><span class="s1"> 
        </span><span class="s2">if </span><span class="s1">freshness_lifetime &gt; current_age: 
            logger.debug(</span><span class="s3">'The response is &quot;fresh&quot;, returning cached response'</span><span class="s1">) 
            logger.debug(</span><span class="s3">'%i &gt; %i'</span><span class="s2">, </span><span class="s1">freshness_lifetime</span><span class="s2">, </span><span class="s1">current_age) 
            </span><span class="s2">return </span><span class="s1">resp 
 
        </span><span class="s5"># we're not fresh. If we don't have an Etag, clear it out</span><span class="s1"> 
        </span><span class="s2">if </span><span class="s3">'etag' </span><span class="s2">not in </span><span class="s1">headers: 
            logger.debug( 
                </span><span class="s3">'The cached response is &quot;stale&quot; with no etag, purging'</span><span class="s1"> 
            ) 
            self.cache.delete(cache_url) 
 
        </span><span class="s5"># return the original handler</span><span class="s1"> 
        </span><span class="s2">return False</span><span class="s1"> 
 
    </span><span class="s2">def </span><span class="s1">conditional_headers(self</span><span class="s2">, </span><span class="s1">request): 
        cache_url = self.cache_url(request.url) 
        resp = self.serializer.loads(request</span><span class="s2">, </span><span class="s1">self.cache.get(cache_url)) 
        new_headers = {} 
 
        </span><span class="s2">if </span><span class="s1">resp: 
            headers = CaseInsensitiveDict(resp.headers) 
 
            </span><span class="s2">if </span><span class="s3">'etag' </span><span class="s2">in </span><span class="s1">headers: 
                new_headers[</span><span class="s3">'If-None-Match'</span><span class="s1">] = headers[</span><span class="s3">'ETag'</span><span class="s1">] 
 
            </span><span class="s2">if </span><span class="s3">'last-modified' </span><span class="s2">in </span><span class="s1">headers: 
                new_headers[</span><span class="s3">'If-Modified-Since'</span><span class="s1">] = headers[</span><span class="s3">'Last-Modified'</span><span class="s1">] 
 
        </span><span class="s2">return </span><span class="s1">new_headers 
 
    </span><span class="s2">def </span><span class="s1">cache_response(self</span><span class="s2">, </span><span class="s1">request</span><span class="s2">, </span><span class="s1">response</span><span class="s2">, </span><span class="s1">body=</span><span class="s2">None</span><span class="s1">): 
        </span><span class="s0">&quot;&quot;&quot; 
        Algorithm for caching requests. 
 
        This assumes a requests Response object. 
        &quot;&quot;&quot;</span><span class="s1"> 
        </span><span class="s5"># From httplib2: Don't cache 206's since we aren't going to</span><span class="s1"> 
        </span><span class="s5">#                handle byte range requests</span><span class="s1"> 
        cacheable_status_codes = [</span><span class="s4">200</span><span class="s2">, </span><span class="s4">203</span><span class="s2">, </span><span class="s4">300</span><span class="s2">, </span><span class="s4">301</span><span class="s1">] 
        </span><span class="s2">if </span><span class="s1">response.status </span><span class="s2">not in </span><span class="s1">cacheable_status_codes: 
            logger.debug( 
                </span><span class="s3">'Status code %s not in %s'</span><span class="s2">,</span><span class="s1"> 
                response.status</span><span class="s2">,</span><span class="s1"> 
                cacheable_status_codes 
            ) 
            </span><span class="s2">return</span><span class="s1"> 
 
        response_headers = CaseInsensitiveDict(response.headers) 
 
        </span><span class="s5"># If we've been given a body, our response has a Content-Length, that</span><span class="s1"> 
        </span><span class="s5"># Content-Length is valid then we can check to see if the body we've</span><span class="s1"> 
        </span><span class="s5"># been given matches the expected size, and if it doesn't we'll just</span><span class="s1"> 
        </span><span class="s5"># skip trying to cache it.</span><span class="s1"> 
        </span><span class="s2">if </span><span class="s1">(body </span><span class="s2">is not None and</span><span class="s1"> 
                </span><span class="s3">&quot;content-length&quot; </span><span class="s2">in </span><span class="s1">response_headers </span><span class="s2">and</span><span class="s1"> 
                response_headers[</span><span class="s3">&quot;content-length&quot;</span><span class="s1">].isdigit() </span><span class="s2">and</span><span class="s1"> 
                int(response_headers[</span><span class="s3">&quot;content-length&quot;</span><span class="s1">]) != len(body)): 
            </span><span class="s2">return</span><span class="s1"> 
 
        cc_req = self.parse_cache_control(request.headers) 
        cc = self.parse_cache_control(response_headers) 
 
        cache_url = self.cache_url(request.url) 
        logger.debug(</span><span class="s3">'Updating cache with response from &quot;%s&quot;'</span><span class="s2">, </span><span class="s1">cache_url) 
 
        </span><span class="s5"># Delete it from the cache if we happen to have it stored there</span><span class="s1"> 
        no_store = </span><span class="s2">False</span><span class="s1"> 
        </span><span class="s2">if </span><span class="s1">cc.get(</span><span class="s3">'no-store'</span><span class="s1">): 
            no_store = </span><span class="s2">True</span><span class="s1"> 
            logger.debug(</span><span class="s3">'Response header has &quot;no-store&quot;'</span><span class="s1">) 
        </span><span class="s2">if </span><span class="s1">cc_req.get(</span><span class="s3">'no-store'</span><span class="s1">): 
            no_store = </span><span class="s2">True</span><span class="s1"> 
            logger.debug(</span><span class="s3">'Request header has &quot;no-store&quot;'</span><span class="s1">) 
        </span><span class="s2">if </span><span class="s1">no_store </span><span class="s2">and </span><span class="s1">self.cache.get(cache_url): 
            logger.debug(</span><span class="s3">'Purging existing cache entry to honor &quot;no-store&quot;'</span><span class="s1">) 
            self.cache.delete(cache_url) 
 
        </span><span class="s5"># If we've been given an etag, then keep the response</span><span class="s1"> 
        </span><span class="s2">if </span><span class="s1">self.cache_etags </span><span class="s2">and </span><span class="s3">'etag' </span><span class="s2">in </span><span class="s1">response_headers: 
            logger.debug(</span><span class="s3">'Caching due to etag'</span><span class="s1">) 
            self.cache.set( 
                cache_url</span><span class="s2">,</span><span class="s1"> 
                self.serializer.dumps(request</span><span class="s2">, </span><span class="s1">response</span><span class="s2">, </span><span class="s1">body=body)</span><span class="s2">,</span><span class="s1"> 
            ) 
 
        </span><span class="s5"># Add to the cache any 301s. We do this before looking that</span><span class="s1"> 
        </span><span class="s5"># the Date headers.</span><span class="s1"> 
        </span><span class="s2">elif </span><span class="s1">response.status == </span><span class="s4">301</span><span class="s1">: 
            logger.debug(</span><span class="s3">'Caching permanant redirect'</span><span class="s1">) 
            self.cache.set( 
                cache_url</span><span class="s2">,</span><span class="s1"> 
                self.serializer.dumps(request</span><span class="s2">, </span><span class="s1">response) 
            ) 
 
        </span><span class="s5"># Add to the cache if the response headers demand it. If there</span><span class="s1"> 
        </span><span class="s5"># is no date header then we can't do anything about expiring</span><span class="s1"> 
        </span><span class="s5"># the cache.</span><span class="s1"> 
        </span><span class="s2">elif </span><span class="s3">'date' </span><span class="s2">in </span><span class="s1">response_headers: 
            </span><span class="s5"># cache when there is a max-age &gt; 0</span><span class="s1"> 
            </span><span class="s2">if </span><span class="s1">cc </span><span class="s2">and </span><span class="s1">cc.get(</span><span class="s3">'max-age'</span><span class="s1">): 
                </span><span class="s2">if </span><span class="s1">cc[</span><span class="s3">'max-age'</span><span class="s1">].isdigit() </span><span class="s2">and </span><span class="s1">int(cc[</span><span class="s3">'max-age'</span><span class="s1">]) &gt; </span><span class="s4">0</span><span class="s1">: 
                    logger.debug(</span><span class="s3">'Caching b/c date exists and max-age &gt; 0'</span><span class="s1">) 
                    self.cache.set( 
                        cache_url</span><span class="s2">,</span><span class="s1"> 
                        self.serializer.dumps(request</span><span class="s2">, </span><span class="s1">response</span><span class="s2">, </span><span class="s1">body=body)</span><span class="s2">,</span><span class="s1"> 
                    ) 
 
            </span><span class="s5"># If the request can expire, it means we should cache it</span><span class="s1"> 
            </span><span class="s5"># in the meantime.</span><span class="s1"> 
            </span><span class="s2">elif </span><span class="s3">'expires' </span><span class="s2">in </span><span class="s1">response_headers: 
                </span><span class="s2">if </span><span class="s1">response_headers[</span><span class="s3">'expires'</span><span class="s1">]: 
                    logger.debug(</span><span class="s3">'Caching b/c of expires header'</span><span class="s1">) 
                    self.cache.set( 
                        cache_url</span><span class="s2">,</span><span class="s1"> 
                        self.serializer.dumps(request</span><span class="s2">, </span><span class="s1">response</span><span class="s2">, </span><span class="s1">body=body)</span><span class="s2">,</span><span class="s1"> 
                    ) 
 
    </span><span class="s2">def </span><span class="s1">update_cached_response(self</span><span class="s2">, </span><span class="s1">request</span><span class="s2">, </span><span class="s1">response): 
        </span><span class="s0">&quot;&quot;&quot;On a 304 we will get a new set of headers that we want to 
        update our cached value with, assuming we have one. 
 
        This should only ever be called when we've sent an ETag and 
        gotten a 304 as the response. 
        &quot;&quot;&quot;</span><span class="s1"> 
        cache_url = self.cache_url(request.url) 
 
        cached_response = self.serializer.loads( 
            request</span><span class="s2">,</span><span class="s1"> 
            self.cache.get(cache_url) 
        ) 
 
        </span><span class="s2">if not </span><span class="s1">cached_response: 
            </span><span class="s5"># we didn't have a cached response</span><span class="s1"> 
            </span><span class="s2">return </span><span class="s1">response 
 
        </span><span class="s5"># Lets update our headers with the headers from the new request:</span><span class="s1"> 
        </span><span class="s5"># http://tools.ietf.org/html/draft-ietf-httpbis-p4-conditional-26#section-4.1</span><span class="s1"> 
        </span><span class="s5">#</span><span class="s1"> 
        </span><span class="s5"># The server isn't supposed to send headers that would make</span><span class="s1"> 
        </span><span class="s5"># the cached body invalid. But... just in case, we'll be sure</span><span class="s1"> 
        </span><span class="s5"># to strip out ones we know that might be problmatic due to</span><span class="s1"> 
        </span><span class="s5"># typical assumptions.</span><span class="s1"> 
        excluded_headers = [ 
            </span><span class="s3">&quot;content-length&quot;</span><span class="s2">,</span><span class="s1"> 
        ] 
 
        cached_response.headers.update( 
            dict((k</span><span class="s2">, </span><span class="s1">v) </span><span class="s2">for </span><span class="s1">k</span><span class="s2">, </span><span class="s1">v </span><span class="s2">in </span><span class="s1">response.headers.items() 
                 </span><span class="s2">if </span><span class="s1">k.lower() </span><span class="s2">not in </span><span class="s1">excluded_headers) 
        ) 
 
        </span><span class="s5"># we want a 200 b/c we have content via the cache</span><span class="s1"> 
        cached_response.status = </span><span class="s4">200</span><span class="s1"> 
 
        </span><span class="s5"># update our cache</span><span class="s1"> 
        self.cache.set( 
            cache_url</span><span class="s2">,</span><span class="s1"> 
            self.serializer.dumps(request</span><span class="s2">, </span><span class="s1">cached_response)</span><span class="s2">,</span><span class="s1"> 
        ) 
 
        </span><span class="s2">return </span><span class="s1">cached_response 
</span></pre>
</body>
</html>